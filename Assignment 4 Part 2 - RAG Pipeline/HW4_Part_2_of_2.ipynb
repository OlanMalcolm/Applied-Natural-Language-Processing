{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summer 2025 Applied NLP Homework 4 (Part 2 of 2)\n",
    "\n",
    "## Instructors: Dr. Mahdi Roozbahani, Wafa Louhichi, Dr. Nimisha Roy\n",
    "\n",
    "## Deadline: July 25th, 11:59PM AoE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Honor Code and Assignment Deadline\n",
    "<!-- No changes needed on the below section -->\n",
    "* No unapproved extension of the deadline is allowed. Late submission will lead to 0 credit. \n",
    "\n",
    "* Discussion is encouraged on Ed as part of the Q/A. However, all assignments should be done individually.\n",
    "\n",
    "* <font color='darkred'>Plagiarism is a **serious offense**. You are responsible for completing your own work. You are not allowed to copy and paste, or paraphrase, or submit materials created or published by others, as if you created the materials. All materials submitted must be your own.</font>\n",
    "\n",
    "* <font color='darkred'>All incidents of suspected dishonesty, plagiarism, or violations of the Georgia Tech Honor Code will be subject to the institute’s Academic Integrity procedures. If we observe any (even small) similarities/plagiarisms detected by Gradescope or our TAs, **WE WILL DIRECTLY REPORT ALL CASES TO OSI**, which may, unfortunately, lead to a very harsh outcome. **Consequences can be severe, e.g., academic probation or dismissal, grade penalties, a 0 grade for assignments concerned, and prohibition from withdrawing from the class.**\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions for the assignment \n",
    "\n",
    "<!-- No changes needed on the below section -->\n",
    "- This entire assignment will be autograded through Gradescope:\n",
    "    - **REQUIRED - Homework 4 Q7**: Submit the Q7 files to this section\n",
    "    - **Bonus - Homework 4 Q8**: Submit the Q8 files to this section\n",
    "    - **See HW4 Part 1 for Q1-Q5 (Required) and Q6 (Bonus)**\n",
    "\n",
    "\n",
    "- We provided you different .py files and we added libraries in those files please DO NOT remove those lines and add your code after those lines. Note that these are the only allowed libraries that you can use for the homework.\n",
    "\n",
    "- You will submit your implemented .py files to the corresponding homework section on Gradescope. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab Setup (Optional for running on Colab)\n",
    "If you choose to work on the assignment on Google Colab, the following cell may help get you set up. You may need to right click on the Applied NLP folder and `Add shortcut to Drive`. You do not have to run this cell if you are working on the notebook locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mount google drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "# # You may need to create an Applied_NLP/HW#/hw#_code/ folder\n",
    "# %cd '/content/drive/MyDrive/Applied_NLP/HW#/hw#_code/'\n",
    "\n",
    "# ## This wraps output text according to the window size\n",
    "# from IPython.display import HTML, display\n",
    "\n",
    "# def set_css():\n",
    "#   display(HTML('''\n",
    "#   <style>\n",
    "#     pre {\n",
    "#         white-space: pre-wrap;\n",
    "#     }\n",
    "#   </style>\n",
    "#   '''))\n",
    "# get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we will explore a RAG Pipeline for the task of answering multiple choice questions about NLP research papers. In our RAG pipeline, we will use FAISS as our vector store and HuggingFace for our models. You may explore different models to see what best fits this task, but for autograding purposes, Gradescope will automatically assign the LLM model for grading consistency. \n",
    "\n",
    "As a bonus exercise, you will deploy the Q7 RAG pipeline using Ollama and Flask instead of HuggingFace. Question 8 will need to be implemented locally, it is not compatible with Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables and Points Distribution\n",
    "\n",
    "### Q7: RAG Pipeline [12.5pts]\n",
    "- **7.1 Implementing the Retriever** [2pts] Deliverables: <font color = 'green'>retriever.py</font>\n",
    "\n",
    "    - [1.5pt] loadDocuments\n",
    "\n",
    "    - [1.5pt] splitDocuments\n",
    "\n",
    "    - [3pt] createRetriever\n",
    "- **7.2 Implementing the RAG Chain** [2.5pts] Deliverables: <font color = 'green'>rag_chain.py</font>\n",
    "\n",
    "    - \\_\\_init\\_\\_\n",
    "\n",
    "    - init_retriever_system\n",
    "\n",
    "    - [2pt] createPrompt\n",
    "\n",
    "    - [4.5pts] createRAGChain\n",
    " \n",
    "### Q8: Deploy RAG and LLM Using Ollama and Flask [5.5pts Bonus Extra Credit] \n",
    "- **8.1 Using Ollama with RAG** [0pts] Deliverables: <font color = 'green'>rag_chain.py</font>\n",
    "\n",
    "    - [0pt] set_ollama_only\n",
    "\n",
    "- **8.2 Deploying an LLM and using it with RAG** [0pts] Deliverables: <font color = 'green'>llm_app.py and rag_chain.py</font>\n",
    "\n",
    "    - [0pt] generate\n",
    "\n",
    "    - [0pts] set_flask_ollama\n",
    " \n",
    "- **8.3 Deploying RAG using FlaskG** [0pts] Deliverables: <font color = 'green'>rag_app.py and hashed_answers.json</font>\n",
    "\n",
    "    - [0pt] query\n",
    "\n",
    "    - [5.5pts] running ta_pipeline to get hashed_answers.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "**Please checkout the environment_setup.md file to create the environment for this homework.** This notebook is tested under the package versions noted in the Library Imports cell output below, and the corresponding packages can be downloaded from [miniconda](https://docs.conda.io/en/latest/miniconda.html). \n",
    "\n",
    "In the .py files please implement the functions that have `raise NotImplementedError`, and after you finish the coding, please delete or comment out `raise NotImplementedError`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If using Google Colab, uncomment this cell and run\n",
    "# !pip install numpy==1.26\n",
    "# !pip install transformers==4.44\n",
    "# !pip install ollama\n",
    "# !pip install langchain==0.2.14\n",
    "# !pip install langchain-community==0.2.12\n",
    "# !pip install sentence-transformers==3.0.1\n",
    "# !pip install faiss-cpu==1.8.0.post1\n",
    "# !pip install pypdf==4.3.1\n",
    "# !pip install python-dotenv\n",
    "# !pip install langchain-openai langchain-core==0.2.41\n",
    "# !pip install gradio_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rutomo/opt/anaconda3/envs/nlp_hw4_gai_summer_2025/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version information\n",
      "python: 3.10.18 (main, Jun  5 2025, 08:13:51) [Clang 14.0.6 ]\n",
      "numpy: 1.24.3\n",
      "langchain: 0.2.14\n",
      "langchain_community: 0.2.12\n",
      "sentence_transformers: 3.0.1\n",
      "pypdf: 4.3.1\n",
      "langchain_core: 0.2.43\n",
      "langchain_openai: 0.1.25\n",
      "python-dotenv: 1.1.1\n",
      "ollama: 0.5.1\n"
     ]
    }
   ],
   "source": [
    "#Import the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import langchain\n",
    "import langchain_community\n",
    "import langchain_core\n",
    "import langchain_openai\n",
    "import sentence_transformers\n",
    "import pypdf\n",
    "import dotenv\n",
    "import importlib.metadata\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "print('Version information')\n",
    "\n",
    "print('python: {}'.format(sys.version))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('langchain: {}'.format(langchain.__version__))\n",
    "print('langchain_community: {}'.format(langchain_community.__version__))\n",
    "print('sentence_transformers: {}'.format(sentence_transformers.__version__))\n",
    "print('pypdf: {}'.format(pypdf.__version__))\n",
    "print('langchain_core: {}'.format(langchain_core.__version__))\n",
    "print('langchain_openai: {}'.format(importlib.metadata.version(\"langchain-openai\")))\n",
    "print('python-dotenv: {}'.format(importlib.metadata.version(\"python-dotenv\")))\n",
    "print('ollama: {}'.format(importlib.metadata.version(\"ollama\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7: Introduction to Retrieval Augmented Generation (RAG) [4.5pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State-of-the-art Large Language Models (LLM's) are trained on an enormous corpus of textual data (taken from webpages, articles, books etc.), and they store a wide range of general knowledge in their parameters. While they are able to perform well on tasks that require general knowledge, they tend to struggle on tasks that require information that wasn't present in the training data. For instance, LLM's may struggle on tasks that require knowledge of domain-specific information, or even up-to-date information. \n",
    "\n",
    "It is very important to overcome this problem, because it is undesirable to get a non-answer from the LLM, and potentially even dangerous if the LLM begins to hallucinate (i.e ramble on with an answer that seems believable but is factually inaccurate). Therefore, it is essential to bridge the gap between the LLM's general knowledge, and other domain-specific or up-to-date information in order to help the LLM generate responses that are contextual and factually accurate, while reducing the chances of hallucinations.\n",
    "\n",
    "There are two effective ways of accomplishing this:\n",
    "1. <strong>Fine-tuning the LLM on the domain-specific/proprietary/new data</strong>: \n",
    "    <ul>\n",
    "        <li>By fine-tuning the model, it can be made suitable for the task.</li>\n",
    "        <li>However, this comes with some limitations. It is compute-intensive, expensive and not agile (it's not realistic to fine-tune an LLM with the new data coming in everyday)</li>\n",
    "    </ul>\n",
    "2. <strong>Retrieval Augmented Generation (RAG)</strong>: \n",
    "    <ul>\n",
    "        <li><a href='https://arxiv.org/abs/2005.11401'>This technique</a> provides the LLM with contextual information from an external knowledge source that can be updated more easily.</li>\n",
    "        <li>It allows the LLM to generate more contextual, and factually accurate responses by allowing it to dynamically access information from an external knowledge source.</li>\n",
    "    </ul> \n",
    "\n",
    "<center><img src=\"data/images/rag-architecture.png\" alt=\"drawing\" width=\"700\" align='center'></center>\n",
    "<center>Basic RAG Architecture</center>\n",
    "\n",
    "\n",
    "The RAG pipeline consists of the following steps:\n",
    "<ol>\n",
    "    <li><strong>Retrieval</strong>: The user's query is used to retrieve the relevant contextual information from the external knowledge source. The external knowledge source is a vector store that contains the embeddings of the documents that contain the proprietary/domain-specific data. The user query is embedded into the same vector space as these documents, and a similarity search is performed in this embedding space to retrieve the documents that are most similar to the user's query. These retrieved documents make up the context that the LLM will consider in order to produce factually accurate responses. </li>\n",
    "    <li><strong>Augmentation</strong>: The user's query is augmented with the retrieved contextual information to form the prompt. The prompt will also usually include instructions to the LLM for performing the task. There is an entire sub-field known as Prompt Engineering, that is dedicated to fine-tuning the prompt so as to get the best possible response from the LLM, and you will get some experience with it in <strong>7.2</strong> while implementing the RAG Chain. </li>\n",
    "    <li><strong>Generation</strong>: The constructed prompt, including the instructions to the LLM, the user query and the retrieved context, is fed to the LLM to generate a response that is comprehensible and factually correct.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 : Implementing the Retriever [3pts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will implement the Retriever that will be used in the RAG Chain. First we will load the documents to be used as the basis of the external knowledge source. Then we will split the documents into smaller chunks to be used with the vector database. Finally we will create the Retriever object that will be used with our RAG Chain to retrieve relevant document chunks as additional contextual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 - 7.1.2 : Loading and Pre-processing data: Document Loaders and Text Splitters [1.5pts each]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain provides <a href='https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/'>several classes</a> to load data into Document objects. There are classes to load data from HTML files, PDF files, JSON files, CSV files, file directories etc. In this homework, we'll be using Langchain's <a href='https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/#pypdf-directory'>PyPDFDirectoryLoader</a> to load PDF's from a directory. A Document object is a dictionary that stores the text and metadata about each document.\n",
    "\n",
    "Once the data has been loaded into Document(s), it's common to split the documents into smaller chunks. This is done because when a user inputs a query to the system, the retriever will return the most relevant documents, which will be augmented to the prompt that is sent to the LLM. There are limits to the length of this prompt, since it must fit into the LLM's context window. Therefore, it's common to split documents into smaller chunks, so that only the retrieved relevant chunks are inserted into the prompt. Langchain offers several <a href='https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/'>TextSplitters</a> to perform the document splitting. In this homework, we use the RecursiveCharacterTextSplitter which is a way to split a document into several chunks in a manner such that related pieces of text are kept together in a chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **retriever.py** file complete the following functions:\n",
    "\n",
    "* **loadDocuments**\n",
    "* **splitDocuments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Tests for Loading and Splitting Documents \n",
      "\n",
      "Your load documents works as expected: True\n",
      "Your split documents works as expected: True\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from retriever import Retriever\n",
    "from local_tests.retriever_test import Retriever_Test\n",
    "\n",
    "local_test = Retriever_Test()\n",
    "stu_retriever = Retriever()\n",
    "num_chunks_to_query = 2\n",
    "\n",
    "print('Local Tests for Loading and Splitting Documents \\n')\n",
    "\n",
    "# Local test for load documents\n",
    "output_documents = stu_retriever.loadDocuments(data_dir='./data/papers/')\n",
    "load_test = (len(output_documents) == local_test.load_documents_len)\n",
    "print('Your load documents works as expected:', load_test)\n",
    "\n",
    "# Local test for split documents\n",
    "output_chunks = stu_retriever.splitDocuments(output_documents, chunk_size=700, chunk_overlap=50)\n",
    "split_test = (len(output_chunks) == local_test.split_documents_len)\n",
    "print('Your split documents works as expected:', split_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents type: <class 'list'>\n",
      "documents[0] type: <class 'langchain_core.documents.base.Document'>\n",
      "documents length: 104\n",
      "\n",
      "Content of documents[0]:\n",
      " page_content='Improving Language Understanding\n",
      "by Generative Pre-Training\n",
      "Alec Radford\n",
      "OpenAI\n",
      "alec@openai.comKarthik Narasimhan\n",
      "OpenAI\n",
      "karthikn@openai.comTim Salimans\n",
      "OpenAI\n",
      "tim@openai.comIlya Sutskever\n",
      "OpenAI\n",
      "ilyasu@openai.com\n",
      "Abstract\n",
      "Natural language understanding comprises a wide range of diverse tasks such\n",
      "as textual entailment, question answering, semantic similarity assessment, and\n",
      "document classiﬁcation. Although large unlabeled text corpora are abundant,\n",
      "labeled data for learning these speciﬁc tasks is scarce, making it challenging for\n",
      "discriminatively trained models to perform adequately. We demonstrate that large\n",
      "gains on these tasks can be realized by generative pre-training of a language model\n",
      "on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each\n",
      "speciﬁc task. In contrast to previous approaches, we make use of task-aware input\n",
      "transformations during ﬁne-tuning to achieve effective transfer while requiring\n",
      "minimal changes to the model architecture. We demonstrate the effectiveness of\n",
      "our approach on a wide range of benchmarks for natural language understanding.\n",
      "Our general task-agnostic model outperforms discriminatively trained models that\n",
      "use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the\n",
      "state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute\n",
      "improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on\n",
      "question answering (RACE), and 1.5% on textual entailment (MultiNLI).\n",
      "1 Introduction\n",
      "The ability to learn effectively from raw text is crucial to alleviating the dependence on supervised\n",
      "learning in natural language processing (NLP). Most deep learning methods require substantial\n",
      "amounts of manually labeled data, which restricts their applicability in many domains that suffer\n",
      "from a dearth of annotated resources [ 61]. In these situations, models that can leverage linguistic\n",
      "information from unlabeled data provide a valuable alternative to gathering more annotation, which\n",
      "can be time-consuming and expensive. Further, even in cases where considerable supervision\n",
      "is available, learning good representations in an unsupervised fashion can provide a signiﬁcant\n",
      "performance boost. The most compelling evidence for this so far has been the extensive use of pre-\n",
      "trained word embeddings [ 10,39,42] to improve performance on a range of NLP tasks [ 8,11,26,45].\n",
      "Leveraging more than word-level information from unlabeled text, however, is challenging for two\n",
      "main reasons. First, it is unclear what type of optimization objectives are most effective at learning\n",
      "text representations that are useful for transfer. Recent research has looked at various objectives\n",
      "such as language modeling [ 44], machine translation [ 38], and discourse coherence [ 22], with each\n",
      "method outperforming the others on different tasks.1Second, there is no consensus on the most\n",
      "effective way to transfer these learned representations to the target task. Existing techniques involve\n",
      "a combination of making task-speciﬁc changes to the model architecture [ 43,44], using intricate\n",
      "learning schemes [ 21] and adding auxiliary learning objectives [ 50]. These uncertainties have made\n",
      "it difﬁcult to develop effective semi-supervised learning approaches for language processing.\n",
      "1https://gluebenchmark.com/leaderboard\n",
      "Preprint. Work in progress.' metadata={'source': 'data/papers/gpt.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Load the PDF documents\n",
    "from retriever import Retriever\n",
    "retriever = Retriever()\n",
    "documents = retriever.loadDocuments(data_dir='./data/papers/')\n",
    "\n",
    "print('documents type:', type(documents)) # python list\n",
    "print('documents[0] type:', type(documents[0]))   # Document object\n",
    "print('documents length:', len(documents))   # each page is loaded as a separate document (104 pages -> 104 documents)\n",
    "\n",
    "print('\\nContent of documents[0]:\\n', documents[0])   # the first document object, containing 'page_content' and 'metadata' fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document_chunks type: <class 'list'>\n",
      "document_chunks[0] type: <class 'langchain_core.documents.base.Document'>\n",
      "document_chunks length: 701\n",
      "\n",
      "\n",
      " page_content='Improving Language Understanding\n",
      "by Generative Pre-Training\n",
      "Alec Radford\n",
      "OpenAI\n",
      "alec@openai.comKarthik Narasimhan\n",
      "OpenAI\n",
      "karthikn@openai.comTim Salimans\n",
      "OpenAI\n",
      "tim@openai.comIlya Sutskever\n",
      "OpenAI\n",
      "ilyasu@openai.com\n",
      "Abstract\n",
      "Natural language understanding comprises a wide range of diverse tasks such\n",
      "as textual entailment, question answering, semantic similarity assessment, and\n",
      "document classiﬁcation. Although large unlabeled text corpora are abundant,\n",
      "labeled data for learning these speciﬁc tasks is scarce, making it challenging for\n",
      "discriminatively trained models to perform adequately. We demonstrate that large' metadata={'source': 'data/papers/gpt.pdf', 'page': 0}\n",
      "\n",
      "\n",
      " page_content='gains on these tasks can be realized by generative pre-training of a language model\n",
      "on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each\n",
      "speciﬁc task. In contrast to previous approaches, we make use of task-aware input\n",
      "transformations during ﬁne-tuning to achieve effective transfer while requiring\n",
      "minimal changes to the model architecture. We demonstrate the effectiveness of\n",
      "our approach on a wide range of benchmarks for natural language understanding.\n",
      "Our general task-agnostic model outperforms discriminatively trained models that\n",
      "use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the' metadata={'source': 'data/papers/gpt.pdf', 'page': 0}\n",
      "\n",
      "\n",
      " page_content='state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute\n",
      "improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on\n",
      "question answering (RACE), and 1.5% on textual entailment (MultiNLI).\n",
      "1 Introduction\n",
      "The ability to learn effectively from raw text is crucial to alleviating the dependence on supervised\n",
      "learning in natural language processing (NLP). Most deep learning methods require substantial\n",
      "amounts of manually labeled data, which restricts their applicability in many domains that suffer\n",
      "from a dearth of annotated resources [ 61]. In these situations, models that can leverage linguistic' metadata={'source': 'data/papers/gpt.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Split the documents into smaller chunks\n",
    "document_chunks = retriever.splitDocuments(documents)\n",
    "\n",
    "print('document_chunks type:', type(document_chunks)) # python list\n",
    "print('document_chunks[0] type:', type(document_chunks[0])) # Document object\n",
    "print('document_chunks length:', len(document_chunks)) # you should observe that each document (corresponding to a page in a PDF) has been split into several chunks\n",
    "\n",
    "for chunk in document_chunks[:3]:   # displaying the first 3 chunks\n",
    "    print('\\n\\n', chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before split, there were 104 documents, with average size equal to 4149.2307692307695.\n",
      "After split, there were 701 documents (chunks), with average size equal to 624.5235378031384.\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# analysis of chunk size\n",
    "def compute_avg_chunk_size(chunks):\n",
    "    return sum([len(chunk.page_content) for chunk in chunks])/len(chunks)\n",
    "\n",
    "print(f'Before split, there were {len(documents)} documents, with average size equal to {compute_avg_chunk_size(documents)}.')\n",
    "print(f'After split, there were {len(document_chunks)} documents (chunks), with average size equal to {compute_avg_chunk_size(document_chunks)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.3 : Creating the Vector Database and Retrieval System: Embedding models and Vector Stores [3pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://python.langchain.com/v0.2/docs/integrations/text_embedding/'>Langchain provides several embedding models</a>. An embedding model converts a piece of natural language (e.g. a token) into a vector embedding. You should have some knowledge of word embedding models such as Word2Vec and GloVe. We will be using Huggingface's BGE Embedding models, which are the one of the best open-source embedding models <a href='https://python.langchain.com/v0.2/docs/integrations/text_embedding/bge_huggingface/'>(according to Langchain)</a>. The model we use (BAAI/bge-small-en-v1.5) has 384-dimensional embedding vectors.\n",
    "\n",
    "Once we have our embedding model, we need to create the vectorstore by embedding all of the document chunks into the vector space. A retriever will be used to retrieve the document chunks from the vectorstore that are most similar to the user's query using efficient similarity search algorithms. Langchain offers <a href='https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/'>several vectorstores</a>, and we will be using Facebook's AI Similarity Search (FAISS). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **retriever.py** file complete the following functions:\n",
    "\n",
    "* **createRetriever**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Tests for Retriever \n",
      "\n",
      "Your retriever embeddings returns the expected shape: True\n",
      "Your retriever returns the expected number of chunks: True\n",
      "Your retriever returns chunks from the expected relevant documents: True\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from retriever import Retriever\n",
    "from local_tests.retriever_test import Retriever_Test\n",
    "\n",
    "local_test = Retriever_Test()\n",
    "stu_retriever = Retriever()\n",
    "num_chunks_to_query = 2\n",
    "\n",
    "print('Local Tests for Retriever \\n')   # ensure that the local tests for the loading and splitting documents pass\n",
    "\n",
    "output_documents = stu_retriever.loadDocuments(data_dir='./data/papers/')\n",
    "output_chunks = stu_retriever.splitDocuments(output_documents, chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "# Local test for retriever embeddings\n",
    "output_retrieval_system = stu_retriever.createRetriever(output_chunks, num_chunks_to_return=num_chunks_to_query)\n",
    "student_embedding_model = stu_retriever.huggingface_embeddings\n",
    "output_embedding = np.array(student_embedding_model.embed_query(output_chunks[0].page_content))\n",
    "embedding_shape = output_embedding.shape[0]\n",
    "\n",
    "if (student_embedding_model.model_name == local_test.model_name): \n",
    "    embedding_shape_test = (embedding_shape == local_test.embedding_size)\n",
    "    print('Your retriever embeddings returns the expected shape:', embedding_shape_test)\n",
    "else:\n",
    "    print('You are free to choose the embedding model to use for the best results (provided it works with Gradescope),')\n",
    "    print(f'but we can only locally test the embedding shape (no value testing) if using the {local_test.model_name} model')\n",
    "    print('and the RecursiveCharacterTextSplitter.')\n",
    "\n",
    "# Local test for chunk relevance\n",
    "output_retrieved_chunks = output_retrieval_system.invoke(local_test.relevance_prompt)\n",
    "returned_count_test = (len(output_retrieved_chunks) == num_chunks_to_query)\n",
    "relevance_test = True\n",
    "for chunk in output_retrieved_chunks:\n",
    "    if local_test.relevant_pdf not in chunk.metadata['source']:\n",
    "        relevance_test = False\n",
    "print('Your retriever returns the expected number of chunks:', returned_count_test)\n",
    "print('Your retriever returns chunks from the expected relevant documents:', relevance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# create the retriever\n",
    "retrieval_system = retriever.createRetriever(document_chunks)\n",
    "embedding_model = retriever.huggingface_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the embedding:  (384,)\n",
      "Sample embedding of a document chunk:  [-1.86509583e-02 -1.27523541e-02 -4.42757010e-02  1.99839976e-02\n",
      "  4.54358570e-02 -8.86677764e-03  6.96528179e-04  1.14407567e-02\n",
      "  3.35411355e-02 -3.01743709e-02 -2.33880375e-02 -6.56098872e-02\n",
      "  6.75567240e-02  7.66655132e-02  6.66777343e-02  2.27699522e-02\n",
      " -4.07686736e-03  2.88760904e-02 -1.48760751e-02 -1.81919839e-02\n",
      "  2.06040591e-02 -5.04119545e-02  6.91428606e-04 -1.10212546e-02\n",
      " -5.39466664e-02  4.52645356e-03  3.32243694e-03 -3.94856669e-02\n",
      " -1.03674065e-02 -2.04585850e-01  2.24794075e-02  2.93982495e-02\n",
      "  8.67721438e-02  6.67661009e-03  6.90001808e-03  3.32516544e-02\n",
      " -3.38732228e-02  1.15591940e-02  2.11533215e-02 -1.29593629e-02\n",
      " -1.53322490e-02 -1.74208246e-02 -5.63682523e-03 -4.73322459e-02\n",
      "  1.91294141e-02 -2.30338313e-02 -1.88581925e-02 -4.31183949e-02\n",
      " -7.51457363e-02 -2.49210652e-02 -5.70337102e-02 -8.07441622e-02\n",
      "  1.16802193e-03  1.10151945e-02  1.16324676e-02  4.70592342e-02\n",
      "  5.32041416e-02  2.85158046e-02  2.27537248e-02  4.49682884e-02\n",
      " -1.34335103e-04  3.08912117e-02 -1.23522103e-01  7.22467974e-02\n",
      " -1.07976366e-02  3.05269603e-02 -7.86783174e-02  2.33130977e-02\n",
      " -1.67446788e-02  2.33003870e-02  5.09207770e-02  7.37933489e-03\n",
      "  4.13252190e-02  4.83025648e-02  9.01658740e-03  3.62157002e-02\n",
      "  1.49394730e-02 -1.39702652e-02  1.61957778e-02 -1.83977429e-02\n",
      "  3.43015566e-02  2.46729217e-02  2.79864576e-02 -4.05168831e-02\n",
      " -1.08976057e-02 -2.75043901e-02 -1.39573943e-02 -3.16464752e-02\n",
      " -4.86016786e-03 -9.66046646e-04  9.55773424e-03 -8.12951010e-03\n",
      " -8.25568009e-03  9.51153506e-03 -5.41732796e-02 -2.29727030e-02\n",
      "  2.35080123e-02 -1.44206639e-03  6.33836957e-03  3.88460428e-01\n",
      " -1.35775581e-02 -3.46611179e-02  1.76615396e-03 -7.63483569e-02\n",
      "  1.51953066e-03 -8.26519355e-03 -7.48483976e-03 -9.38477460e-03\n",
      " -1.72134377e-02 -3.73017378e-02 -6.27783388e-02 -2.12347228e-02\n",
      " -4.32159193e-02  4.16625757e-03  3.32176425e-02 -1.06023327e-02\n",
      "  1.38116300e-01  6.91054156e-03 -3.11832763e-02  1.36276172e-03\n",
      " -1.89824700e-02 -8.16358253e-03 -3.33643295e-02 -4.26942594e-02\n",
      "  2.82013211e-02 -4.23293896e-02  2.27176901e-02  8.78501236e-02\n",
      "  6.62924256e-03  5.01226448e-02  2.30405238e-02  2.17892621e-02\n",
      " -7.68816844e-02  7.29017099e-03  4.52175029e-02 -1.27178598e-02\n",
      "  2.65576690e-02 -2.22701114e-02 -2.91624051e-02  3.17545161e-02\n",
      " -1.32442163e-02 -9.78688058e-03  1.54130394e-02 -6.65428955e-03\n",
      " -1.27087981e-01  8.82134959e-02  3.53958784e-03 -3.65761807e-03\n",
      " -5.27974144e-02 -8.24230909e-02  2.76711322e-02  2.16411334e-02\n",
      "  2.20773593e-02 -5.17291613e-02  3.23101394e-02  3.71310450e-02\n",
      "  8.10433328e-02 -5.73089495e-02 -5.45471348e-02 -2.79804617e-02\n",
      "  2.53188722e-02 -3.78623642e-02 -3.11116129e-02  1.20084651e-01\n",
      "  2.84145437e-02 -6.74086511e-02 -1.09924860e-02 -2.89463941e-02\n",
      " -2.82145794e-02 -5.95488548e-02  7.56318718e-02  1.79406609e-02\n",
      "  1.19691994e-02  6.52340651e-02 -4.59580636e-03  2.59029549e-02\n",
      " -5.61034046e-02 -2.98020407e-03  1.17275417e-02  9.40581132e-03\n",
      "  4.17152885e-03 -8.56803805e-02  1.39653245e-02  5.94612472e-02\n",
      " -3.06106284e-02 -6.47855848e-02  3.24597792e-03 -1.91760324e-02\n",
      "  2.31693015e-02 -7.67058833e-03  6.39854791e-03  1.19446684e-02\n",
      " -1.00631760e-02  3.30933668e-02 -4.51581553e-03  1.46871917e-02\n",
      "  2.19285022e-02 -1.46456426e-02 -2.34485306e-02 -2.56355256e-02\n",
      " -2.65176799e-02  5.69110885e-02  2.00881390e-03 -1.29999258e-02\n",
      " -4.50357310e-02 -3.15868296e-02  3.51172425e-02  1.33909471e-02\n",
      "  6.66953921e-02  3.34205255e-02 -5.08037545e-02 -1.02353271e-03\n",
      " -3.78044918e-02  1.73979867e-02 -8.24301783e-03 -4.25665639e-02\n",
      "  2.31032539e-02  1.42896911e-02  4.12611989e-03  5.70875918e-03\n",
      " -3.04878578e-02  2.06732769e-02 -3.74390301e-03 -2.94348896e-01\n",
      "  2.14339932e-03  2.63784863e-02 -7.78368907e-03  1.18271159e-02\n",
      " -6.05057701e-02 -1.23289106e-02 -3.06728575e-03  5.06163500e-02\n",
      "  1.68905742e-02 -2.92067621e-02  6.12196419e-03 -1.32256048e-02\n",
      " -1.73538793e-02  2.61710584e-02  1.84787586e-02  6.58813193e-02\n",
      "  2.50516087e-02 -1.47157684e-02  3.95851359e-02  6.94321766e-02\n",
      "  2.58380547e-02  4.68731597e-02 -1.31101757e-01  1.37179326e-02\n",
      " -5.42274080e-02  1.61416054e-01 -6.63123606e-03  5.56874946e-02\n",
      " -3.17687243e-02  4.08624206e-03  3.11744921e-02 -4.51919585e-02\n",
      " -7.48772845e-02  6.94223642e-02 -2.56111603e-02  7.67421629e-03\n",
      "  2.48990338e-02 -3.10284528e-03  9.05390922e-03  1.99773461e-02\n",
      "  2.51397397e-02 -1.24634067e-02 -5.53960763e-02 -4.80974168e-02\n",
      " -3.34807001e-02 -3.23283747e-02 -4.25675176e-02 -3.84431827e-04\n",
      "  7.22498596e-02 -1.93598885e-02  5.18565997e-02  4.02324833e-02\n",
      "  2.69414429e-02 -5.22457175e-02 -1.48850493e-03 -1.04216002e-01\n",
      "  2.61844019e-03 -4.01861854e-02 -4.25052829e-02  2.52573378e-02\n",
      " -2.07167305e-03 -1.58331748e-02 -7.24130422e-02  1.43326828e-02\n",
      "  1.30050927e-02 -4.36213948e-02  1.17886979e-02  9.61581431e-03\n",
      " -8.22508149e-03 -5.14335409e-02  6.83367997e-02 -6.13381229e-02\n",
      "  4.14316058e-02  4.24908958e-02  4.35940847e-02  1.07833855e-02\n",
      " -6.47117794e-02 -4.37343009e-02  1.80456713e-02  1.04462869e-01\n",
      "  4.84838672e-02  8.88547525e-02 -4.01380099e-02  4.57015224e-02\n",
      "  4.98962263e-03  7.08964169e-02 -3.12833637e-02  3.01222000e-02\n",
      "  5.33072352e-02  4.22526337e-02  3.76056768e-02 -1.52883760e-03\n",
      " -8.76358449e-02  3.08791064e-02 -5.90588385e-03 -2.38102213e-01\n",
      "  2.55738609e-02  4.25244756e-02  4.63105924e-02  1.82178821e-02\n",
      "  5.32119498e-02  1.71896890e-02 -4.08422574e-02 -8.26735049e-03\n",
      "  1.74509420e-03 -5.09858206e-02  4.84998338e-02  1.32082850e-02\n",
      " -2.38939971e-02 -5.74468747e-02  1.71895735e-02  9.01217088e-02\n",
      " -2.34443676e-02  2.73107141e-02 -2.75580510e-02  3.52901630e-02\n",
      "  2.44525652e-02  1.60876378e-01 -4.10040170e-02  2.73354650e-02\n",
      " -6.76351860e-02 -2.51044128e-02 -8.31023976e-02  7.31359876e-04\n",
      " -7.32065318e-03  1.98166221e-02 -2.36756802e-02  8.78411606e-02\n",
      "  4.21130657e-02  2.47080512e-02  6.54765442e-02  3.55905294e-02\n",
      "  7.12773530e-03  1.99536532e-02 -2.09468752e-02  1.27548594e-02\n",
      " -1.74988643e-04 -2.56931931e-02 -2.42940765e-02 -1.89117510e-02\n",
      "  6.63478374e-02  4.12648134e-02 -5.80352135e-02 -5.96640185e-02\n",
      " -2.25030836e-02  1.86344031e-02  6.50116382e-03 -3.97259975e-03\n",
      "  4.71076220e-02  4.97827008e-02  1.06808497e-02  2.11676843e-02\n",
      " -4.33245003e-02 -4.18471247e-02 -4.30862606e-02 -3.16936597e-02\n",
      " -3.11042182e-02  2.62527671e-02  4.51781228e-02 -4.89583835e-02]\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# sample embedding for a document chunk\n",
    "sample_embedding = np.array(embedding_model.embed_query(document_chunks[0].page_content))\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What novel techniques did the 'Attention is all you need' paper introduce?\n",
      "\n",
      "RETRIEVED CHUNKS: \n",
      "page_content='2020. URL https://arxiv.org/abs/2004.14366 .\n",
      "[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
      "Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,\n",
      "S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\n",
      "Information Processing Systems 30 , pages 5998–6008. Curran Associates, Inc., 2017. URL\n",
      "http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .\n",
      "[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David\n",
      "Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.' metadata={'source': 'data/papers/rag.pdf', 'page': 14} \n",
      "\n",
      "page_content='networks , vol. 3361, no. 10, p. 1995, 1995.\n",
      "[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\n",
      "Gomez, Ł. Kaiser, and I. Polosukhin, “Attention Is All You Need,”\n",
      "Advances in neural information processing systems , vol. 30, 2017.\n",
      "[19] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\n",
      "Y . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning\n",
      "with a unified text-to-text transformer,” The Journal of Machine\n",
      "Learning Research , vol. 21, no. 1, pp. 5485–5551, 2020.\n",
      "[20] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT:\n",
      "Pre-training of deep bidirectional transformers for language\n",
      "understanding,” in Proceedings of NAACL-HLT , 2019.' metadata={'source': 'data/papers/gen_ai_survey.pdf', 'page': 17} \n",
      "\n",
      "page_content='reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].' metadata={'source': 'data/papers/attention.pdf', 'page': 1} \n",
      "\n",
      "page_content='for natural language understanding. In Proceedings\n",
      "of the 2018 EMNLP Workshop BlackboxNLP: An-\n",
      "alyzing and Interpreting Neural Networks for NLP ,\n",
      "pages 353–355.\n",
      "Wei Wang, Ming Yan, and Chen Wu. 2018b. Multi-\n",
      "granularity hierarchical attention fusion networks\n",
      "for reading comprehension and question answering.\n",
      "InProceedings of the 56th Annual Meeting of the As-\n",
      "sociation for Computational Linguistics (Volume 1:\n",
      "Long Papers) . Association for Computational Lin-\n",
      "guistics.\n",
      "Alex Warstadt, Amanpreet Singh, and Samuel R Bow-\n",
      "man. 2018. Neural network acceptability judg-\n",
      "ments. arXiv preprint arXiv:1805.12471 .\n",
      "Adina Williams, Nikita Nangia, and Samuel R Bow-' metadata={'source': 'data/papers/bert.pdf', 'page': 11} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "QUESTION: List the metrics were used to compare GloVe vectors with other embedding methods such as Word2Vec?\n",
      "\n",
      "RETRIEVED CHUNKS: \n",
      "page_content='chine), a single iteration takes 14 minutes. See\n",
      "Fig. 4 for a plot of the learning curve.\n",
      "4.7 Model Analysis: Comparison with\n",
      "word2vec\n",
      "A rigorous quantitative comparison of GloVe with\n",
      "word2vec is complicated by the existence of\n",
      "many parameters that have a strong effect on per-\n",
      "formance. We control for the main sources of vari-\n",
      "ation that we identiﬁed in Sections 4.4 and 4.5 by\n",
      "setting the vector length, context window size, cor-\n",
      "pus, and vocabulary size to the conﬁguration men-\n",
      "tioned in the previous subsection.\n",
      "The most important remaining variable to con-\n",
      "trol for is training time. For GloVe, the rele-\n",
      "vant parameter is the number of training iterations.' metadata={'source': 'data/papers/glove.pdf', 'page': 8} \n",
      "\n",
      "page_content='The GloVe model outperforms all other methods\n",
      "on all evaluation metrics, except for the CoNLL\n",
      "test set, on which the HPCA method does slightly\n",
      "better. We conclude that the GloVe vectors are\n",
      "useful in downstream NLP tasks, as was ﬁrst\n",
      "8We use the same parameters as above, except in this case\n",
      "we found 5 negative samples to work slightly better than 10.' metadata={'source': 'data/papers/glove.pdf', 'page': 7} \n",
      "\n",
      "page_content='GloVe: Global Vectors for Word Representation\n",
      "Jeffrey Pennington, Richard Socher, Christopher D. Manning\n",
      "Computer Science Department, Stanford University, Stanford, CA 94305\n",
      "jpennin@stanford.edu, richard@socher.org, manning@stanford.edu\n",
      "Abstract\n",
      "Recent methods for learning vector space\n",
      "representations of words have succeeded\n",
      "in capturing ﬁne-grained semantic and\n",
      "syntactic regularities using vector arith-\n",
      "metic, but the origin of these regularities\n",
      "has remained opaque. We analyze and\n",
      "make explicit the model properties needed\n",
      "for such regularities to emerge in word\n",
      "vectors. The result is a new global log-\n",
      "bilinear regression model that combines\n",
      "the advantages of the two major model' metadata={'source': 'data/papers/glove.pdf', 'page': 0} \n",
      "\n",
      "page_content='type of weighting scheme proposed in our model.\n",
      "Table 3 shows results on ﬁve different word\n",
      "similarity datasets. A similarity score is obtained\n",
      "from the word vectors by ﬁrst normalizing each\n",
      "feature across the vocabulary and then calculat-\n",
      "ing the cosine similarity. We compute Spearman’s\n",
      "rank correlation coefﬁcient between this score and\n",
      "the human judgments. CBOW∗denotes the vec-\n",
      "tors available on the word2vec website that are\n",
      "trained with word and phrase vectors on 100B\n",
      "words of news data. GloVe outperforms it while\n",
      "using a corpus less than half the size.\n",
      "Table 4 shows results on the NER task with the\n",
      "CRF-based model. The L-BFGS training termi-' metadata={'source': 'data/papers/glove.pdf', 'page': 7} \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# demonstration of the retriever finding the relevant document chunks\n",
    "questions = [\n",
    "    \"What novel techniques did the 'Attention is all you need' paper introduce?\",\n",
    "    \"List the metrics were used to compare GloVe vectors with other embedding methods such as Word2Vec?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f'QUESTION: {question}' + '\\n')\n",
    "\n",
    "    retrieved_chunks = retrieval_system.invoke(question)\n",
    "    print('RETRIEVED CHUNKS: ')\n",
    "    for chunk in retrieved_chunks:\n",
    "        print(chunk, '\\n')\n",
    "\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 : Implementing the RAG Chain [6.5pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building applications with Large Language Models (LLM's) requires permissions from the LLM provider via an API key. There are two types of LLM models available for use: \n",
    "<ol>\n",
    "    <li>Open-source models, which are usually smaller models with lesser capabilities (e.g. Llama created by FaceBook, Flan-T5 created by Google)</li>\n",
    "    <li>Proprietary models, which are usually larger with better performace (e.g. GPT-4o by OpenAI, Gemini-1.5 by Google etc.), but aren't free to use.</li>\n",
    "</ol>\n",
    "\n",
    "In this homework, you will use an open-source HuggingFace LLM, which is free to use. In order to use a HuggingFace LLM, there are some steps that you first need to complete:\n",
    "<ul>\n",
    "    <li>Create a <a href= \"https://huggingface.co/\">Huggingface</a> Account</li>\n",
    "    <li>Create a new <a href='https://huggingface.co/settings/tokens'>API Access Token</a> with the <strong>WRITE</strong> Token Type. <strong>Save this access token in a secure place and do not share it with others.</strong> For this assignment you will <strong>save the token to the READ_ME file and follow the instructions to create a <a href=\"https://dev.to/jakewitcher/using-env-files-for-environment-variables-in-python-applications-55a1\">.env file</a> file.</strong> </li>\n",
    "    <li>Accept the terms and conditions. This step may be required for certain models like <a href= 'https://huggingface.co/mistralai/Mistral-7B-v0.1'>mistralai/Mistral-7B-v0.1</a>. You can view the status of your model request <a href='https://huggingface.co/settings/gated-repos'>here</a></li>\n",
    "</ul>\n",
    "\n",
    "We also need to host our model on HuggingFace Spaces. To do this: \n",
    "<ul>\n",
    "  <li>Go to: <a href= https://huggingface.co/spaces>Spaces</a></li>\n",
    "  <li>Click \"+ New Space\" in the top right</li>\n",
    "  <li>Fill in: \n",
    "    <ul>\n",
    "      <li>Space name: Name of your choice</li>\n",
    "      <li>SDK: Choose Gradio (blank template)</li>\n",
    "      <li>Space hardware: Choose CPU basic (free)</li>\n",
    "      <li>Visibility: Private</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Click: Create Space</li>\n",
    "  <li>Select Files in the top right which will take you to a new page</li>\n",
    "  <li>Select + Contribute in the top right</li>\n",
    "  <li>Select Upload files</li>\n",
    "  <li>Add the files in hf_spaces folder provided to your space, modifying them appropriately for your models</li>\n",
    "</ul>\n",
    "\n",
    "For this section you will need to modify both the `READ_ME` and `rag_chain.py`files. \n",
    "\n",
    "In the `READ_ME` file, complete the following:\n",
    "* Set `HUGGINGFACE_API_KEY`: Obtain your Huggingface API key and follow the instructions in the READ_ME file\n",
    "* Set `GRADIO_SPACE_NAME`: Set your Gradio space name after you have created your space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.0 : Choosing an LLM and Initalizing the Retriever System [No Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **rag_chain.py** file complete the following functions:\n",
    "* **\\_\\_init\\_\\_**: This function is given to you and uses `google/flan-t5-small` as the default model. We have found that Google's Flan Models and some of Meta's Llama models work well for this question, however, you are free to play around with other models.\n",
    "* **init_retriever_system**: No points are associated for implementing this function, but it is an integral component to the RAG Chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from rag_chain import RAG_Chain\n",
    "from local_tests.retriever_test import Retriever_Test\n",
    "from langchain_core.vectorstores.base import VectorStoreRetriever\n",
    "\n",
    "# Instantiate + configure the RAG_Chain class to use a HF-hosted Gradio Space via a custom LLM wrapper\n",
    "rag_chain = RAG_Chain(data_dir='./data/papers/', llm_type=\"gradio_flan\")\n",
    "local_test = Retriever_Test()\n",
    "\n",
    "# Printing the LLM used\n",
    "print(rag_chain.llm) \n",
    "\n",
    "# Local test for RAG retriever system\n",
    "\n",
    "# Check that retriever is of expected type:\n",
    "rag_retriever = rag_chain.retriever_system\n",
    "type_test = isinstance(rag_retriever, VectorStoreRetriever)\n",
    "print('Your retriever is of type VectorStoreRetriever:', type_test)\n",
    "\n",
    "# Check that the retriever retrieves relevant chunks\n",
    "output_retrieved_chunks = rag_retriever.invoke(local_test.relevance_prompt)\n",
    "relevance_test = True\n",
    "for chunk in output_retrieved_chunks:\n",
    "    if local_test.relevant_pdf not in chunk.metadata['source']:\n",
    "        relevance_test = False\n",
    "print('Your retriever returns chunks from the expected relevant documents:', relevance_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 : Formatting the Prompt with PromptTemplates [2pts]\n",
    "\n",
    "Prompts are a set of instructions that are given to an LLM in order to guide it to produce responses that are coherent, contextual and relevant. It usually takes several edits and changes to the prompt until the LLM produces the desirable response. This process is called Prompt Engineering, and you will get some experience with it here. Once you have obtained a prompt that you like, it's common practice to save it as a template for any time you want to use this prompt again. This is done with PromptTemplates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **rag_chain.py** file complete the following function:\n",
    "\n",
    "* **createPrompt**\n",
    "\n",
    "<br>\n",
    "This function takes as an input parameter, a dictionary that stores a question along with 4 possible answer choices. For example, here is what the input parameter could look like: \n",
    "\n",
    "{ <br>\n",
    "   &emsp; 'question': \"What is the main contribution of the Transformer architecture?\", <br>\n",
    "   &emsp; 'A': \"It introduces convolutional layers for sequence tasks.\", <br>\n",
    "   &emsp; 'B': \"It improves word embeddings using context.\", <br>\n",
    "   &emsp; 'C': \"It removes recurrence and uses self-attention mechanisms.\", <br>\n",
    "   &emsp; 'D': \"It uses RNNs for language modeling.\" <br>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# printing the empty prompt template \n",
    "prompt_template = rag_chain.createPrompt(question={'question': \"\", \"A\": \"\", \"B\": \"\", \"C\": \"\", \"D\": \"\"})\n",
    "\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# printing the formatted prompt with the question. This is what will be passed to the LLM.\n",
    "from local_tests.rag_test import RAG_Test\n",
    "tests = RAG_Test()\n",
    "\n",
    "prompt_with_question = rag_chain.createPrompt(question=tests.question1)\n",
    "print(prompt_with_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 Creating Chains [4.5pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LangChain, \"chains\" are a core concept designed to manage and streamline interactions with language models. They allow you to create sequences of operations where the output of one step can be used as the input for the next. This is particularly useful for building complex workflows and applications that involve multiple stages of processing.\n",
    "\n",
    "Langchain offers a RetrievalQA chain, which combines the retriever module with a QA chain (short for Question-Answering). The retriever is used to retrieve relevant documents from the vectorstore, and the QA chain answers questions based on the retrieved documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **rag_chain.py** file complete the following function:\n",
    "\n",
    "* **createRAGChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "qa_chain = rag_chain.createRAGChain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from local_tests.rag_test import RAG_Test\n",
    "\n",
    "tests = RAG_Test()\n",
    "\n",
    "print('Local Tests for end-to-end RAG pipeline', '\\n\\n')\n",
    "\n",
    "questions = tests.local_test_questions\n",
    "answers = tests.local_test_answers\n",
    "\n",
    "correct = 0\n",
    "total = len(questions)\n",
    "for i in range(len(questions)):\n",
    "    # create the prompt with a question\n",
    "    prompt_with_question = rag_chain.createPrompt(question=questions[i])\n",
    "    print(prompt_with_question)\n",
    "\n",
    "    # query the LLM\n",
    "    response = qa_chain(prompt_with_question)\n",
    "\n",
    "    print('Answer selected by the LLM:', response['result'])\n",
    "    print('Correct answer:', answers[i])\n",
    "\n",
    "    if response['result'] == answers[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(f'{correct}/{total} questions answered correctly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8: Hosting and Deploying LLM and RAG [5.5pts Bonus Extra Credit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you'll engage in a hands-on exercise exploring Ollama-hosted and Flask Ollama-hosted LLMs within the RAG pipeline you created in the previous question. You'll then deploy the RAG system using a Flask container, simulating a real-world LLM Pipeline deployment.\n",
    "\n",
    "This exercise provides an opportunity to compare different hosting methods:\n",
    "\n",
    "- **Cloud-based LLMs** (Hugging Face Hub or Spaces) offload the burden of compute but require external API calls.\n",
    "- **Local LLMs** (Ollama) offer greater control over data, cost, and customizations, but compute is limited by our hardware.\n",
    "- **Network-hosted LLMs** (Flask Ollama) allow for remote access within a private infrastructure, which is useful for on-premise deployments in industries like healthcare and finance where data privacy is critical.\n",
    "  \n",
    "In many real-world applications, LLMs are accessed through APIs rather than used directly. In Question 7, you worked with Hugging Face’s API via Hugging Face Spaces. Now, you’ll deploy the same RAG pipeline but using Ollama and Flask.\n",
    "\n",
    "While we provide structured code scaffolding, we encourage you to examine the implementation carefully to understand how the various components interact. We hope this introduction gives you the necessary tools to experiment with different deployment strategies. \n",
    "\n",
    "**IMPORTANT: This question may need to be completed locally, it is not compatible with Google Colab. Using a GPU may also output different results than expected.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 : Using Ollama with RAG [No Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will get started with Ollama and use the Ollama LLM in the RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 Getting Started with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download [Ollama](https://ollama.com/) and pull the following model:\n",
    "- `llama3.2`\n",
    "  \n",
    "You may find the [ollama-python Documentation](https://github.com/ollama/ollama-python) (prerequisites section) and [Quickstart Guide](https://github.com/ollama/ollama/blob/main/README.md#quickstart) helpful. Make sure the ollama server is running on your machine prior to running the local test below.\n",
    "\n",
    "**You will need to keep ollama running to complete the next sections, do not terminate it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=(true | false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Ollama server returned the expected response: True\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "import ta_pipeline as tap\n",
    "from local_tests.q8_test import Q8_Test\n",
    "\n",
    "local_test = Q8_Test()\n",
    "\n",
    "# Check that Ollama returns the expected response\n",
    "response = tap.query_ollama(local_test.ollama_query)\n",
    "response_check = response == local_test.ollama_response\n",
    "print('Your Ollama server returned the expected response:', response_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2 Using Ollama with RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **rag_chain.py** file complete the following function:\n",
    "* **set_ollama_only**: Initialize the LLM using the Ollama wrapper (see [documentation](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ollama.Ollama.html)) and set the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Info:\n",
      " \u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'llama3.2', 'format': None, 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': None, 'num_thread': None, 'num_predict': None, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None, 'raw': None}\n",
      "\n",
      "Your Ollama LLM in the RAG system returned the expected response: True\n",
      "\n",
      "QUERY: True or False, a bird is a bat.\n",
      "EXPECTED RESPONSE: False. Birds and bats are two distinct groups of animals that belong to different classes (Aves for birds and Chiroptera for bats) and have many physical and behavioral differences.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from local_tests.q8_test import Q8_Test\n",
    "from rag_chain import RAG_Chain\n",
    "\n",
    "# Local test for RAG using Ollama LLM\n",
    "\n",
    "# Check that Ollama works with rag_chain.py\n",
    "rag_chain_oo = RAG_Chain(data_dir='./data/papers/', llm_type=\"ollama_only\", init_retriever=False)\n",
    "rag_chain_oo.llm.temperature = 0\n",
    "local_test = Q8_Test()\n",
    "\n",
    "# Printing the LLM used\n",
    "print(f\"LLM Info:\\n {rag_chain_oo.llm}\\n\") \n",
    "\n",
    "# Check that RAG returns the expected response\n",
    "response = rag_chain_oo.query_the_llm(local_test.ollama_rag_query)\n",
    "response_check = response == local_test.ollama_rag_response\n",
    "print('Your Ollama LLM in the RAG system returned the expected response:', response_check)\n",
    "print(\"\\nQUERY:\", local_test.ollama_rag_query)\n",
    "print(\"EXPECTED RESPONSE:\", local_test.ollama_rag_response)\n",
    "print(f\"Your Response: {response}\" if not response_check else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 : Deploying an LLM and using it with RAG [No Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will deploy the Ollama LLM from 8.1.1 by containerizing the LLM using Flask. While Ollama works locally, Flask can expose the LLM for network use. We will query into this Flask Ollama LLM by reusing Langchain's OpenAI wrapper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1: Using Flask to Containerize the Ollama LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **llm_app.py** file complete the following function:\n",
    "* **generate**: Extract the data from the Flask message request, complete the request to the Ollama server using the extracted data, format the response as required per the OpenAI wrapper and return.\n",
    "\n",
    "In your terminal or command prompt, run the llm_app.py file before running the local test cell below.\n",
    "\n",
    "**You will need to keep llm_app.py running to complete the next section, do not terminate it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Flask Ollama server returned the expected response: True\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "import ta_pipeline as tap\n",
    "from local_tests.q8_test import Q8_Test\n",
    "\n",
    "local_test = Q8_Test()\n",
    "\n",
    "# Local test for Flask Ollama server\n",
    "\n",
    "#Check that Ollama returns the expected response\n",
    "response = tap.query_flask_ollama(local_test.ollama_query)\n",
    "response_check = response == local_test.ollama_response\n",
    "print('Your Flask Ollama server returned the expected response:', response_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 Using Flask Ollama with RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **rag_chain.py** file complete the following function:\n",
    "* **set_flask_ollama**: Initialize the LLM using the OpenAI wrapper and set the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Info:\n",
      " \u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'llama3.2', 'temperature': 0, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'seed': None, 'logprobs': None, 'max_tokens': 256}\n",
      "\n",
      "Your Flask Ollama LLM in the RAG system returned the expected response: True\n",
      "\n",
      "QUERY: True or False, a bird is a bat.\n",
      "EXPECTED RESPONSE: False. Birds and bats are two distinct groups of animals that belong to different classes (Aves for birds and Chiroptera for bats) and have many physical and behavioral differences.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "import ta_pipeline as tap\n",
    "from local_tests.q8_test import Q8_Test\n",
    "from rag_chain import RAG_Chain\n",
    "\n",
    "local_test = Q8_Test()\n",
    "\n",
    "# Local test for RAG using Flask Ollama\n",
    "\n",
    "# Check that Ollama works with rag_chain.py\n",
    "rag_chain_fo = RAG_Chain(data_dir='./data/papers/', llm_type=\"flask_ollama\", init_retriever=False)\n",
    "rag_chain_fo.llm.temperature = 0\n",
    "local_test = Q8_Test()\n",
    "\n",
    "# Printing the LLM used\n",
    "print(f\"LLM Info:\\n {rag_chain_fo.llm}\\n\") \n",
    "\n",
    "# Check that RAG returns the expected response\n",
    "response = rag_chain_fo.query_the_llm(local_test.flask_ollama_rag_query)\n",
    "response_check = response == local_test.flask_ollama_rag_response\n",
    "print('Your Flask Ollama LLM in the RAG system returned the expected response:', response_check)\n",
    "print(\"\\nQUERY:\", local_test.flask_ollama_rag_query)\n",
    "print(\"EXPECTED RESPONSE:\", local_test.flask_ollama_rag_response)\n",
    "print(f\"YOUR RESPONSE: {response}\" if not response_check else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 : Deploying RAG using Flask [5.5pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to deploy the RAG implementation from 8.2 using Flask. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1 Deploying RAG Local Test [No Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **rag_app.py** file complete the following function:\n",
    "* **query_rag**: Extract the data from the message request, update the LLM parameters based on the extracted information, query the RAG, format the response and return.\n",
    "\n",
    "In your terminal or command prompt, run the rag_app.py file before running the local test cell below.\n",
    "\n",
    "**You will need to keep rag_app.py running to complete the final TA pipeline test, do not terminate it. You should have Ollama and llm_app.py running before rag_app.py.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Flask RAG system returned the expected response: True\n",
      "\n",
      "QUERY: Does Glove use word similarity?\n",
      "EXPECTED RESPONSE: Yes, according to the text, GloVe uses word-word co-occurrence counts, which implies that it does use word similarity as a basis for generating word representations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "import ta_pipeline as tap\n",
    "from local_tests.q8_test import Q8_Test\n",
    "\n",
    "local_test = Q8_Test()\n",
    "\n",
    "# Local test for Ollama server\n",
    "\n",
    "# Check that flask rag returns the expected response\n",
    "response = tap.query_flask_rag(local_test.flask_rag_query)\n",
    "response_check = response[0][\"text\"] == local_test.flask_rag_response\n",
    "print('Your Flask RAG system returned the expected response:', response_check)\n",
    "print(\"\\nQUERY:\", local_test.flask_rag_query)\n",
    "print(\"EXPECTED RESPONSE:\", local_test.flask_rag_response)\n",
    "print(f\"YOUR RESPONSE: {response[0]['text']}\" if not response_check else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2 Deploying RAG Final Test [5.5 pts Bonus Extra Credit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are able to pass the local test in 8.3.1, proceed to running the test below for the final test of your RAG system. Submit the generated hashed_answers.json file to Gradescope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "%%time\n",
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "import ta_pipeline as tap\n",
    "\n",
    "tap.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🐙 <font color='darkred'>Submit hashed_answers.json to Gradescope</font>\n",
    "**Running the TA pipeline should produce hashed_answers.json. Submit this file to Gradescope along with llm_app.py, rag_app.py, rag_chain.py, and retriever.py.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congrats, you have reached the end of Homework 4 🥳**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
